{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrAC67HQ_sYR"
   },
   "source": [
    "<h1 align='center'> RETAIL SALES FORECASTING </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuMT-ACH_sYW"
   },
   "source": [
    "![Walmart-cash-and-carry.jpg](attachment:Walmart-cash-and-carry.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9vpxb2R_sYX"
   },
   "source": [
    "<b> What is retail sales forecasting ? </b>\n",
    "\n",
    "Forecasting in retail involves evaluating past revenues and consumer behavior over the previous months or year inorder to discern patterns and develop forecasts for the upcoming months. The data is adjusted for seasonal trends, and then the forecast is used as a plan for ordering and stocking products. After fulfillment of current and forthcoming customer purchases and orders, an assessment of the results is compared with previous forecasts, and the entire procedure is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS6VjkWx_sYY"
   },
   "source": [
    "<b> Why is it important ?</b>\n",
    "\n",
    "An accurate forecast that meets the forthcoming consumption demands of customers help retail businesess to maximize and extend profits over the long-term. This forecast permits price adjustments to correspond with the current level of consumer spending patterns, and helps to better maintain and control a sufficient, yet also efficient, inventory that meets the consumer demand without being too excessive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWfUe_yH_sYZ"
   },
   "source": [
    "<b> Scenario </b>\n",
    "\n",
    "We have hierarchical sales data from Walmart, the largest company by revenue in the world, and the goal is to forecast daily sales for the next 28 days. Moreover, the data comes from three US States, California, Texas and Wisconsin, and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events that should be used in order to make the forecast as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5Rs9csS_sYa"
   },
   "source": [
    "## Outline for Sales Forecast\n",
    "\n",
    "1. Framing the Problem\n",
    "2. Preparing the Data\n",
    "3. Exploratory Data Analysis\n",
    "4. Feature Engineering\n",
    "5. Modeling\n",
    "6. Fine-Tuning the Best Model\n",
    "7. Drawing Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV76v9WW_sYa"
   },
   "source": [
    "# 1. Framing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ybZvtn_sYb"
   },
   "source": [
    "When forecasting retail sales we have to consider certain components that help to catch the consumer demand from the past behavior of the consumers. This is driven from two major factors: the quality and the price of the product. On the one hand, we have knowledge of the actual daily prices of the products, so this enables us to calculate the price elasticities, and for the other - the quality ; this will be approximinated by analyzing the past retail sales of the stores with the help of five main components: \n",
    "\n",
    "- `Autoregressive (AR)`   - in the form of linear combinations of past values of the time series up to some number \n",
    "of lags. The products sold in the past help to predict the number of products being sold in the future.\n",
    "\n",
    "\n",
    "- `Moving Average (MA)`    -    in the form of linear combinations that are calculated from the averages of variables in the past. This is the strongest component because it enables to approximate averages from a large set of samples.\n",
    "\n",
    "- `Trend`    -    as in whether we can see a linear or non-linear trend in product sales that help to forecast the future. \n",
    "\n",
    "- `Seasonal`    -    that can be explained happening at a certain particular time with a certain frequency. \n",
    "\n",
    "\n",
    "- `Exogenous` - that come outside the model, for example economic indicators in the form of the unemployment rate, the rate of inflation, growth of the national gross domestic product or the changes in the stock market that can produce a shock in the demand. A minor shock can also be produced with price changes, so we will calculate the price elasticities as a form of competition coming outside the model that affects consumer demand.\n",
    "\n",
    "\n",
    "The components can be explained with a SARIMAX model (i.e. Seasonal Auto-Regressive Integrated Moving Average with eXogenous factors) which takes into account all of the five different components. \n",
    "\n",
    "Which component should be included and how much in the past should we look into, can be statistically tested by having a closer look at the Autocorrelation (ACF) and Partial autocorrelation (PACF) functions of the variables which measure the correlation coefficient between a time-series and the lagged version of itself. \n",
    "\n",
    "Once we would have managed to pinpoint the correct amount of lags for each variable, we could calculate the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) to help choose the model. However, before choosing the lag components, all the variables should be tested for stationarity with the Augmented Dicki-Fuller test, which checks that the variables do not change during time, as if they do change, they cannot be forecasted with a SARIMAX model. The data thus needs to be starionary so that:\n",
    "\n",
    "- Trend is zero,\n",
    "\n",
    "\n",
    "- Variance is constant, \n",
    "\n",
    "\n",
    "- Autocorrelation is constant \n",
    "\n",
    "This means that the distribution of the data shouldn't change during time, but if it would change, the data should be made stationary by for example transforming them either with log-transformations or differencing, which eventually depend whether there is a seasonal or trend component or both involved that needs to be taken care of.\n",
    "\n",
    "<b> Choosing the Model </b>\n",
    "\n",
    "If we would be choosing a SARIMAX model we would need to compute the lag terms separately for each variable and store, and make the variables stationary. This can prove to be quite burdensome with a huge dataset with many variables, but also, and most importantly, could pose some difficulties in capturing all the non-linear effects that affect capturing the signal from the noise in forecasting.\n",
    "\n",
    "Another, more modern approach, comes with the Light Gradient Boosting Machine (LGBM), which is a decision tree based emsemble technique that is able to capture complex, non-linear patterns, that also scale well to large datasets with many variables. The benefit of this approach is that we can incorporate all the variables together, and forecast multiple time series with a single machine learning model instead of choosing the right variables and right amount of lag for each store separately. By using an ensemble technique that utilizes boosting, we are able to take advantage of the average of multiple weaker models. This technique is not only fast to implement, but also often outperforms the more classical type of time series models such as the SARIMAX, and this is also why we are going to choose LGBM as our model.\n",
    "\n",
    "<b> Evaluating the Model </b>\n",
    "\n",
    "We will have a decision tree based model that is able to pick the right features from the set that it will be given to. We will mix this up by given a random sample and random subsample of features to each tree to avoid overfitting. The model will be trained with a validation set and the forecasting will be made with a never-before-seen test set. Finally, we will analyze these results and adjust the accuracy with some fine-tuning of the best model before making the actual 28 day forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nr6NcyrS_sYc",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.395455100Z",
     "start_time": "2023-06-02T13:53:53.987777100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing relevant modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import datetime as dt\n",
    "import pickle, joblib\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Modeling tools\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Ignoring warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plot styling preferences\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "font = {'family' : 'Helvetica',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "mpl.rcParams['figure.figsize'] = 18, 8\n",
    "#pd.set_option('max_columns', None)\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9em_6Q-d_sYe"
   },
   "source": [
    "# 2. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# sales_train_evaluation.zip\n",
    "sales_train_evaluation = 'https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/sales_train_evaluation.zip'\n",
    "# sales_train_validation\n",
    "sales_train_validation = 'https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/sales_train_validation.zip'\n",
    "# calendar\n",
    "calendar = 'https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/calendar.csv'"
   ],
   "metadata": {
    "id": "nkjmuLCIG34-",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.403045100Z",
     "start_time": "2023-06-02T13:53:54.267677300Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#from google.colab import drive\n",
    "#drive.flush_and_unmount()"
   ],
   "metadata": {
    "id": "I5RPfqksJaxl",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.403983Z",
     "start_time": "2023-06-02T13:53:54.272187600Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS5GoQnqHWfn",
    "outputId": "bf32a275-a17f-4649-a3ee-0b9bade8925d",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.403983Z",
     "start_time": "2023-06-02T13:53:54.284213500Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!wget -P data/ https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/sales_train_evaluation.zip\n",
    "#!wget -P data/ https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/sales_train_validation.zip\n",
    "#!wget -P data/ https://github.com/SpyrosPetsis/M5-Forecasting-Accuracy-Kaggle-Competition/raw/main/calendar.csv"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxKEuKxa58S_",
    "outputId": "3f28c806-8b78-4e3f-d14e-989a4ca1e369",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.404986Z",
     "start_time": "2023-06-02T13:53:54.286215Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#!unzip -o data/sales_train_evaluation.zip  -d data/\n",
    "#!unzip -o data/sales_train_validation.zip  -d data/\n",
    "#!unzip -o drive/MyDrive/Share/datasets/sell_prices.csv.zip  -d data/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2SEJ5q9FQRn",
    "outputId": "a9260a8b-a862-4b74-af4f-a0983cbc6648",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:55.404986Z",
     "start_time": "2023-06-02T13:53:54.292722200Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEUqgqjj_sYf",
    "outputId": "a492ac2a-ed3d-411c-8c1c-fee285c47ddf",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.589076100Z",
     "start_time": "2023-06-02T13:53:54.298726800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((30490, 1947), (1969, 14), (6841121, 4))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading datasets\n",
    "\n",
    "sales = pd.read_csv('data/sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "prices = pd.read_csv('data/sell_prices.csv')\n",
    "\n",
    "sales.shape, calendar.shape, prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PPoW2Y0u_sYg",
    "outputId": "0f92d227-ea60-4681-fa45-75b96686bdcf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.596607300Z",
     "start_time": "2023-06-02T13:53:59.587085300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n0       CA    0    0    0    0  ...       2       4       0       0       0   \n1       CA    0    0    0    0  ...       0       1       2       1       1   \n2       CA    0    0    0    0  ...       1       0       2       0       0   \n3       CA    0    0    0    0  ...       1       1       0       4       0   \n4       CA    0    0    0    0  ...       0       0       0       2       1   \n\n   d_1937  d_1938  d_1939  d_1940  d_1941  \n0       0       3       3       0       1  \n1       0       0       0       0       0  \n2       0       2       3       0       1  \n3       1       3       0       2       6  \n4       0       0       2       1       0  \n\n[5 rows x 1947 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>item_id</th>\n      <th>dept_id</th>\n      <th>cat_id</th>\n      <th>store_id</th>\n      <th>state_id</th>\n      <th>d_1</th>\n      <th>d_2</th>\n      <th>d_3</th>\n      <th>d_4</th>\n      <th>...</th>\n      <th>d_1932</th>\n      <th>d_1933</th>\n      <th>d_1934</th>\n      <th>d_1935</th>\n      <th>d_1936</th>\n      <th>d_1937</th>\n      <th>d_1938</th>\n      <th>d_1939</th>\n      <th>d_1940</th>\n      <th>d_1941</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HOBBIES_1_001_CA_1_evaluation</td>\n      <td>HOBBIES_1_001</td>\n      <td>HOBBIES_1</td>\n      <td>HOBBIES</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HOBBIES_1_002_CA_1_evaluation</td>\n      <td>HOBBIES_1_002</td>\n      <td>HOBBIES_1</td>\n      <td>HOBBIES</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HOBBIES_1_003_CA_1_evaluation</td>\n      <td>HOBBIES_1_003</td>\n      <td>HOBBIES_1</td>\n      <td>HOBBIES</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HOBBIES_1_004_CA_1_evaluation</td>\n      <td>HOBBIES_1_004</td>\n      <td>HOBBIES_1</td>\n      <td>HOBBIES</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HOBBIES_1_005_CA_1_evaluation</td>\n      <td>HOBBIES_1_005</td>\n      <td>HOBBIES_1</td>\n      <td>HOBBIES</td>\n      <td>CA_1</td>\n      <td>CA</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1947 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xWl65ul_sYg",
    "outputId": "0eeebde6-2078-44d5-c99f-d6ea8d63e0b5",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.788740700Z",
     "start_time": "2023-06-02T13:53:59.595609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30490 entries, 0 to 30489\n",
      "Columns: 1947 entries, id to d_1941\n",
      "dtypes: int64(1941), object(6)\n",
      "memory usage: 452.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3HY7L8E_sYh"
   },
   "source": [
    "The size of the dataframe is huge with 30,490 rows and 1947 columns. We can see that the the dataset contains unit sales per product and department store, where we have over 30,000 different products that have been sold in a timeframe of 1941 days (5.3 years).\n",
    "\n",
    "The memory usage is also over 450MB as the variables are in dtype int64 form, but this can be reduced, as they do not need to be in such an accurate form that takes so much memory.\n",
    "\n",
    "The first six variables are identity numbers to help track what has been sold and were it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkuXUN2T_sYh",
    "outputId": "fdb0b7da-f33b-4ef9-9c2f-c4771582f943",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.789259800Z",
     "start_time": "2023-06-02T13:53:59.709415800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30490 unique products in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} unique products in the dataset.'.format(sales['id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKWWvLT1_sYh",
    "outputId": "f1af8b8f-0a3a-43ed-c4ab-7024d7c0be89",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.789259800Z",
     "start_time": "2023-06-02T13:53:59.721433500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3049 products with id tags.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} products with id tags.'.format(sales['item_id'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4NX16zv_sYh"
   },
   "source": [
    "We can see that there are 3049 unique products in the dataset which have 10-times more id tags, making a total of 3049. If we look at the first variable, 'id', it is a unique tag for each item in the different stores while the second variable 'item_id' is related to a certain product that is sold in each of the stores. The id is the longer description of a specific item while the item_id is the shorter. For example the first 5 ids show 5 products with their product ids, store ids, state ids and department ids, while the first 5 item ids only show the department id and the product id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cVcHQOqo_sYi",
    "outputId": "167008e4-7190-49f8-c980-8ff070c1d4f2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.790269800Z",
     "start_time": "2023-06-02T13:53:59.737106300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HOBBIES', 'HOUSEHOLD', 'FOODS']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All categories\n",
    "\n",
    "sales['cat_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oSAoK6DP_sYi",
    "outputId": "3f047045-7b54-4180-aee0-e4ba5646c481",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.790269800Z",
     "start_time": "2023-06-02T13:53:59.749129700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HOBBIES_1',\n 'HOBBIES_2',\n 'HOUSEHOLD_1',\n 'HOUSEHOLD_2',\n 'FOODS_1',\n 'FOODS_2',\n 'FOODS_3']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All departments\n",
    "\n",
    "sales['dept_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YyUjoS_W_sYj",
    "outputId": "1d9d1a95-979d-47c3-ced5-2a248d5751bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.791269Z",
     "start_time": "2023-06-02T13:53:59.754644900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['CA_1',\n 'CA_2',\n 'CA_3',\n 'CA_4',\n 'TX_1',\n 'TX_2',\n 'TX_3',\n 'WI_1',\n 'WI_2',\n 'WI_3']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All stores\n",
    "\n",
    "sales['store_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HKEX2M8U_sYj",
    "outputId": "5ba2aa84-3d2a-42f6-d79b-d9b591579004",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:53:59.791269Z",
     "start_time": "2023-06-02T13:53:59.762588100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['CA', 'TX', 'WI']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All States\n",
    "\n",
    "sales['state_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilPoQM9P_sY8"
   },
   "source": [
    "We can see that the 10 stores are spread in 3 States with the 4 stores in California and 3 in Texas and Wisconsin. All the stores three categories of products being sold, food, household and hobbies. These are spread in 7 departments, the food department being the largest with 3 separate departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QLaMguhb_sY9",
    "outputId": "055589b5-6e8b-4e23-dd02-560a7b581b46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.242518300Z",
     "start_time": "2023-06-02T13:53:59.781732400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         date  wm_yr_wk    weekday  wday  month  year    d event_name_1  \\\n0  2011-01-29     11101   Saturday     1      1  2011  d_1          NaN   \n1  2011-01-30     11101     Sunday     2      1  2011  d_2          NaN   \n2  2011-01-31     11101     Monday     3      1  2011  d_3          NaN   \n3  2011-02-01     11101    Tuesday     4      2  2011  d_4          NaN   \n4  2011-02-02     11101  Wednesday     5      2  2011  d_5          NaN   \n\n  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n0          NaN          NaN          NaN        0        0        0  \n1          NaN          NaN          NaN        0        0        0  \n2          NaN          NaN          NaN        0        0        0  \n3          NaN          NaN          NaN        1        1        0  \n4          NaN          NaN          NaN        1        0        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>wm_yr_wk</th>\n      <th>weekday</th>\n      <th>wday</th>\n      <th>month</th>\n      <th>year</th>\n      <th>d</th>\n      <th>event_name_1</th>\n      <th>event_type_1</th>\n      <th>event_name_2</th>\n      <th>event_type_2</th>\n      <th>snap_CA</th>\n      <th>snap_TX</th>\n      <th>snap_WI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-01-29</td>\n      <td>11101</td>\n      <td>Saturday</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>d_1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-01-30</td>\n      <td>11101</td>\n      <td>Sunday</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>d_2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-01-31</td>\n      <td>11101</td>\n      <td>Monday</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2011</td>\n      <td>d_3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011-02-01</td>\n      <td>11101</td>\n      <td>Tuesday</td>\n      <td>4</td>\n      <td>2</td>\n      <td>2011</td>\n      <td>d_4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2011-02-02</td>\n      <td>11101</td>\n      <td>Wednesday</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2011</td>\n      <td>d_5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t2XpFqOf_sY-",
    "outputId": "474bf5f7-788c-4961-c069-a9b04e22bad3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.275594900Z",
     "start_time": "2023-06-02T13:53:59.787758800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1RqCsnmj_sY-",
    "outputId": "ff8c4b4b-fdc5-4362-ab10-8beb17b75a54",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.275594900Z",
     "start_time": "2023-06-02T13:53:59.802516900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[nan,\n 'SuperBowl',\n 'ValentinesDay',\n 'PresidentsDay',\n 'LentStart',\n 'LentWeek2',\n 'StPatricksDay',\n 'Purim End',\n 'OrthodoxEaster',\n 'Pesach End',\n 'Cinco De Mayo',\n \"Mother's day\",\n 'MemorialDay',\n 'NBAFinalsStart',\n 'NBAFinalsEnd',\n \"Father's day\",\n 'IndependenceDay',\n 'Ramadan starts',\n 'Eid al-Fitr',\n 'LaborDay',\n 'ColumbusDay',\n 'Halloween',\n 'EidAlAdha',\n 'VeteransDay',\n 'Thanksgiving',\n 'Christmas',\n 'Chanukah End',\n 'NewYear',\n 'OrthodoxChristmas',\n 'MartinLutherKingDay',\n 'Easter']"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['event_name_1'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tPypO6YB_sY-",
    "outputId": "7bbea52b-5109-47aa-cd96-fb18a53875ce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.277576900Z",
     "start_time": "2023-06-02T13:53:59.812557400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[nan, 'Sporting', 'Cultural', 'National', 'Religious']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['event_type_1'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T2HRZucO_sY-",
    "outputId": "74ce941f-140e-4dd7-cbda-31cd622ffe0c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.277576900Z",
     "start_time": "2023-06-02T13:53:59.822579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[nan, 'Easter', 'Cinco De Mayo', 'OrthodoxEaster', \"Father's day\"]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['event_name_2'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xCthdg84_sY_",
    "outputId": "9077c8ee-ed8b-4be0-a0e9-d8d79877897c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.296165700Z",
     "start_time": "2023-06-02T13:53:59.826600200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[nan, 'Cultural', 'Religious']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['event_type_2'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jQP9gdOA_sY_",
    "outputId": "4e364761-9799-4a1a-b4fc-ef11e8573ea7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.297163700Z",
     "start_time": "2023-06-02T13:53:59.834628100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0, 1]), array([0, 1]), array([0, 1]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['snap_CA'].unique(), calendar['snap_TX'].unique(), calendar['snap_WI'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mh40n2b1_sY_",
    "outputId": "1777216f-5a2e-4e6d-bc24-062f60e636b9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.297163700Z",
     "start_time": "2023-06-02T13:53:59.842152200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0    1319\n 1     650\n Name: snap_CA, dtype: int64,\n 0    1319\n 1     650\n Name: snap_TX, dtype: int64,\n 0    1319\n 1     650\n Name: snap_WI, dtype: int64)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['snap_CA'].value_counts(), calendar['snap_TX'].value_counts(), calendar['snap_WI'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uACp9MXC_sY_"
   },
   "source": [
    "We notice that there are less rows and columns compared to the previous dataset so this takes less memory, but we can still make the changes aswell for the variable dtypes as they do not need to be in this accurate form either.\n",
    "\n",
    "The calendar contains information about the dates and special events that have taken place. These include the specfic day, the day of the week, week, month, and year, with the seasonally occurring events such as the Super Bowl or any other event that occur each year during the same time. SNAP is an acronym for Supplement Nutrition Assistance Program which is a butrition assistance benefit provided by the US federal government. It is aimed for low income families and individuals with an Electronic Benefits Transfer debit card for food purchases.  Both the SNAP and the events should presumably show as increased sales in the stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7KV_AS2a_sZA",
    "outputId": "5ffa1f48-ff91-4b12-9a4f-b866bbf27414",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.298165Z",
     "start_time": "2023-06-02T13:53:59.858674500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  store_id        item_id  wm_yr_wk  sell_price\n0     CA_1  HOBBIES_1_001     11325        9.58\n1     CA_1  HOBBIES_1_001     11326        9.58\n2     CA_1  HOBBIES_1_001     11327        8.26\n3     CA_1  HOBBIES_1_001     11328        8.26\n4     CA_1  HOBBIES_1_001     11329        8.26",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>store_id</th>\n      <th>item_id</th>\n      <th>wm_yr_wk</th>\n      <th>sell_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CA_1</td>\n      <td>HOBBIES_1_001</td>\n      <td>11325</td>\n      <td>9.58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CA_1</td>\n      <td>HOBBIES_1_001</td>\n      <td>11326</td>\n      <td>9.58</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CA_1</td>\n      <td>HOBBIES_1_001</td>\n      <td>11327</td>\n      <td>8.26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CA_1</td>\n      <td>HOBBIES_1_001</td>\n      <td>11328</td>\n      <td>8.26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CA_1</td>\n      <td>HOBBIES_1_001</td>\n      <td>11329</td>\n      <td>8.26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D8EMu9F-_sZA",
    "outputId": "2270b02c-98a2-4e7e-c7a8-4b4a2a61147b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:00.299131400Z",
     "start_time": "2023-06-02T13:53:59.864204700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6841121 entries, 0 to 6841120\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   store_id    object \n",
      " 1   item_id     object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sell_price  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 208.8+ MB\n"
     ]
    }
   ],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds4O46ye_sZA"
   },
   "source": [
    "The prices contain information about the price of the products sold per store and the date. This is also a big dataset as it uses 208MB of memory by having over 6.8M rows and 4 columns. We can again make the change for the dtype variable for this as well, as with the previous datasets as the variable doesn't need to be this accurate so let's start by proceeding to change the dtypes of the variables in order to reduce memory usage for the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzOmTDxS_sZB"
   },
   "source": [
    "## Memory Usage\n",
    "\n",
    "For the challenge of the memory usage, we will create a function that goes through all the different dtypes of integers with the variables in the dataframes and changes the dtype by comparing the minimum and maximum value of a column to the the minimum and maximum value of a dtype integer or float. The dtypes range from 8 to 64 depending on the amount of integers they have, so we will iterate for the types from the smallest to the largest in order to save memory. While the smallest dtype int8 consumes just 1 byte of memory per row, the largest int64 consumes 8 bytes per row which can make a big difference in a dataset that is as large as ours. At the same time we take advantage of iterating through all the dataframes by also changing the objects that are not dates as categories to optimize memory usage even further, and all the dates into datetime objects so that they can be used later for better calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "17mH4qg__sZB",
    "ExecuteTime": {
     "end_time": "2023-06-02T13:54:01.223688100Z",
     "start_time": "2023-06-02T13:53:59.881279200Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m                 df[cols[i]] \u001B[38;5;241m=\u001B[39m df[cols[i]]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df  \n\u001B[0;32m---> 44\u001B[0m sales \u001B[38;5;241m=\u001B[39m \u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m prices \u001B[38;5;241m=\u001B[39m convert(prices)\n\u001B[1;32m     46\u001B[0m calendar \u001B[38;5;241m=\u001B[39m convert(calendar)\n",
      "Cell \u001B[0;32mIn[26], line 37\u001B[0m, in \u001B[0;36mconvert\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     35\u001B[0m         df[cols[i]] \u001B[38;5;241m=\u001B[39m df[cols[i]]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Objects into datetime or category\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m t \u001B[38;5;241m==\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject\u001B[49m:\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cols[i] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     39\u001B[0m         df[cols[i]] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[cols[i]], \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/numpy/__init__.py:305\u001B[0m, in \u001B[0;36m__getattr__\u001B[0;34m(attr)\u001B[0m\n\u001B[1;32m    300\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[1;32m    307\u001B[0m \u001B[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001B[39;00m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;66;03m# the full `numpy.testing` namespace\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# Convert variables into more efficient dtypes and dates into datetime objects\n",
    "\n",
    "def convert(df):\n",
    "    \"\"\"If the type of variable in the dataframe is an integer of float, the function\n",
    "    checks the minimum and maximum value of the cells in a column and changes the dtype\n",
    "    of integers and floats accordingly to smallest possible in order to save memory.\n",
    "    If the type is an object with a date, the function changes this to a datetime object,\n",
    "    else into a category.\n",
    "    \"\"\"\n",
    "    cols = df.dtypes.index.tolist()\n",
    "    types = df.dtypes.values.tolist()\n",
    "    \n",
    "    for i,t in enumerate(types):\n",
    "        # Integers into int8, int16, int32 or int64\n",
    "        if 'int' in str(t):\n",
    "            # Check if minimum and maximum are in the limit of int8\n",
    "            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int8)\n",
    "            # Check if minimum and maximum are in the limit of int16\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int16)\n",
    "            # Check if minimum and maximum are in the limit of int32\n",
    "            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int32)\n",
    "            # Choose int64\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.int64)\n",
    "        # Floats into float16, float32 or float64\n",
    "        elif 'float' in str(t):\n",
    "            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float16)\n",
    "            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float32)\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype(np.float64)\n",
    "        # Objects into datetime or category\n",
    "        elif t == np.object:\n",
    "            if cols[i] == 'date':\n",
    "                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n",
    "            else:\n",
    "                df[cols[i]] = df[cols[i]].astype('category')\n",
    "    return df  \n",
    "\n",
    "sales = convert(sales)\n",
    "prices = convert(prices)\n",
    "calendar = convert(calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAMadR4K_sZB",
    "outputId": "b16343dc-45c1-446d-a01a-9bd6d303f5e6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.496577400Z"
    }
   },
   "outputs": [],
   "source": [
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgnO2FQ6_sZB",
    "outputId": "a3c26a06-d6a6-4fda-c06d-65debd545d06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.498579300Z"
    }
   },
   "outputs": [],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdGsRTh__sZC"
   },
   "source": [
    "## Combining Datasets\n",
    "\n",
    "Now that the dataset variables are in more compact form let's combine the three datasets together. The sales dataframe is in a wide dataframe format as it has all the days as columns, but the other datasets have a long format where the days are as rows. We convert the sales data into long format with the pandas melt function, where we indicate the variables that we want as columns and the variable that we want to transform as rows. This way we create a new column called sales that indicates the number of items that were sold on a particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_Pk_IzH_sZH",
    "outputId": "beaa76b9-d329-4f49-8a0a-e4b1adddfde2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.502703500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Melting the sales dataset from wide format to long\n",
    "\n",
    "df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "             var_name='d', value_name='sales').dropna()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY9OfLfi_sZH"
   },
   "source": [
    "Now all the datasets are in the same format so we can combine them by merging them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuvNybzC_sZH",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.505223600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine sales, calendar and prices by merging all together\n",
    "\n",
    "df = pd.merge(df, calendar, on='d', how='left')\n",
    "df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFQqj2Wj_sZI",
    "outputId": "9f10344e-44e6-41e7-ea78-e68d4935d9cf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.507221600Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5T_h4tVz_sZI",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.509222400Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can delete the sales dataframe to save memory\n",
    "\n",
    "del sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vLekiba_sZI"
   },
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijmnjv_W_sZI"
   },
   "source": [
    "The goal is to forecast 28 days for the total sales of the stores so the key will be to find patterns that could help making the forecast more accurate. We will start by first looking at the sales more generally, and move to a more detailed analysis once we reach the sales figures occurring in different stores. Finally, at the end we will look at the correlations between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9tl47G6_sZJ",
    "outputId": "730e73ea-5bca-4b24-f37a-350de5f51bd1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.511221600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics for sold products\n",
    "\n",
    "df['sales'].describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w59A52n_sZJ",
    "outputId": "f62ebe2e-1959-42a8-d9e7-820bb2cd3c08",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.513348100Z"
    }
   },
   "outputs": [],
   "source": [
    "df['sales'].plot(kind='density', xlim=[0, 1000])\n",
    "plt.title('Density Plot of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K72kBghM_sZJ"
   },
   "source": [
    "We can see that in a bit over 5 years, 59M products have been sold in the 10 Walmart stores. There are some products that sell more than others, but most that get sold are sold only once per day. The statistics and the density plot show that the distribution is concentrated around the mean of zero with a long right-tail just as with a Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucC4z_V_sZJ",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.516348400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating sales revenues\n",
    "\n",
    "df['revenues'] = df['sales'] * df['sell_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYoZNe9F_sZK"
   },
   "source": [
    "## Sale Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWr7U64y_sZK",
    "outputId": "290795ff-7b17-44d4-9aa6-719a2a892918",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.517346100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales per year\n",
    "\n",
    "df.groupby(['year'])['sales'].count().sort_values().plot(kind='barh')\n",
    "plt.title('Sales per Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pzO9mke_sZK"
   },
   "source": [
    "The sales figures looks steady from year to year and there hasn't been a big change in the sales since the best year in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuRx6wBZ_sZK",
    "outputId": "93d6d8d1-d633-4a4d-fb28-bc6c624f3bc9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.519346400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales per month\n",
    "\n",
    "df.groupby(['month'])['sales'].count().sort_values().plot(kind='barh')\n",
    "plt.title('Sales per Month')\n",
    "months = {'0':'Sep','1':'Nov','2':'Jul','3':'Aug','4':'Oct','5':'Dec','6':'Jan','7':'Jun','8':'Feb', '9':'Apr','10':'Mar','11':'May'}\n",
    "plt.yticks(ticks=np.arange(0,12,1), labels=months.values())\n",
    "plt.xlabel('Quantity in Millions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq5d50Xz_sZL"
   },
   "source": [
    "This is more informative than the previous plot as we see a clear pattern occurring where the best sales take place in the beginning of the year from February till May."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rUk3fER_sZL",
    "outputId": "e2075892-5e06-4e1e-8b53-17479d8a2db3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.712411800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales per day\n",
    "\n",
    "df.groupby(['weekday'])['sales'].count().sort_values().plot(kind='bar', rot=360)\n",
    "plt.title('Sales per Day of Week')\n",
    "plt.ylabel('Quantity in Millions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lC6VtzFK_sZM"
   },
   "source": [
    "The best days for sales occur on Friday in the beginning of the weekend, and on Monday in the beginning of the week. Thursday also seems to be a good while Saturday and Sunday have less sales compared to the other days of the week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hji02wnQ_sZM"
   },
   "source": [
    "## State Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bglmMsKQ_sZM",
    "outputId": "300516e4-d4e7-46c8-f617-482da7654a13",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.712411800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in quantity per state\n",
    "\n",
    "df.groupby(['state_id'])['sales'].count().sort_values().plot(kind='barh')\n",
    "plt.title('Sales per State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQL4fYry_sZN",
    "outputId": "677b5930-4211-4aa8-d9fc-ee42644a014d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.712411800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales revenues per state\n",
    "\n",
    "df.groupby(['state_id'])['revenues'].sum().sort_values().plot(kind='barh')\n",
    "plt.title('Sales Revenues per State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys5c_N2f_sZN"
   },
   "source": [
    "The quantity of products sold and sales revenues look similar. California has the biggest sales in quantity and revenues from the three States.\n",
    "\n",
    "## Category Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruOKEj_e_sZN",
    "outputId": "9c1fe326-5405-4334-820e-33184f15e3c9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.712411800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting quantity sold per category\n",
    "\n",
    "df.groupby(['cat_id'])['sales'].count().sort_values().plot(kind='barh')\n",
    "plt.title('Sales per Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuO_xq3m_sZO",
    "outputId": "97d1081b-9354-47f6-9136-cfe2b387f113",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales revenues per category\n",
    "\n",
    "df.groupby(['cat_id'])['revenues'].sum().sort_values().plot(kind='barh')\n",
    "plt.title('Sales Revenues per Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHhPiWsP_sZO"
   },
   "source": [
    "The food category sells the most in quantity and brings the most revenues from the three different categories.\n",
    "\n",
    "## Department Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk5CzQqa_sZO",
    "outputId": "d15dbd53-1902-40d1-b9a5-66c2aae30494",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting quantity sold per department\n",
    "\n",
    "df.groupby(['dept_id'])['sales'].count().sort_values().plot(kind='barh')\n",
    "plt.title('Sales per Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vm2T1id_sZO",
    "outputId": "ee475fb0-8884-46ca-8ac5-22e3f3e61298",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales revenues per department\n",
    "\n",
    "df.groupby(['dept_id'])['revenues'].sum().sort_values().plot(kind='barh')\n",
    "plt.title('Sales Revenues per Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUuaNynu_sZP"
   },
   "source": [
    "The third food department sells the most in quantity and generates the most in revenues from all the 7 different departments. The two household departments sell a large quantity of products but only the first department seems to generate well revenues, as the second food department sells less products but generates more revenues than the second household department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cvp-BQX5_sZP",
    "outputId": "0bdfa18d-b512-4796-90f9-df7c3731a41a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "df.pivot_table(index=['dept_id'], values=['revenues', 'sales'], aggfunc={'revenues': [np.sum, np.mean, np.std],\n",
    "                                                                      'sales': ['count', np.mean, np.std]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAQ6uqXF_sZP"
   },
   "source": [
    "A closer look shows that there is actually a lot of variability between the different products inside the food departments. Let's plot the the sales of the three food departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuWbh9PQ_sZP",
    "outputId": "11f9d962-0c85-4f43-f4e0-195e07ba70cf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'FOODS_1')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 1st Food Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ed2YlpS1_sZQ",
    "outputId": "060f7c8d-fb1b-4996-8cc4-225d435b50f1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.713411700Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'FOODS_2')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 2nd Food Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PdaTdSg_sZQ",
    "outputId": "674da360-7e2f-4410-d5dc-dfe4323d1cb4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.714411300Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'FOODS_3')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 3rd Food Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0ujGWaI_sZf"
   },
   "source": [
    "We can see from the plots that there has been a steady flow of sales throughout the years with some distinct peaks happening in each year. The majority of the sales have occurred during 2011-2014. The first and third department have had a more volatile demand and seem to follow more closely to each other while the second department has as a more steady and lower demand compared to the other two. The first department has peaks occurring at the beginning of the year which could be an indication of a seasonal demand being high in the beginning of the year. The trend also looks slightly upward sloping from 2015 onwards. Let's now check the other departments from hobbies and household categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k24clDtQ_sZf",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.714411300Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'HOUSEHOLD_1')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 1st Household Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHFUB1K2_sZf",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.715440600Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'HOUSEHOLD_2')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 2nd Household Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffAizO9h_sZf"
   },
   "source": [
    "For the household departments we can see an even clearer peaks when a high volume of sales has occurred proabbly because of some promotional events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXsJQpJG_sZg",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.715440600Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'HOBBIES_1')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 1st Hobbies Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMvRmsL7_sZg",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.715440600Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[(df.dept_id == 'HOBBIES_2')][['date', 'sales']].set_index('date').sum(axis=1).plot()\n",
    "plt.title('Sales in the 2nd Hobbies Department')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HunS4iOH_sZg"
   },
   "source": [
    "We can see a similar pattern of more disctinct peaks occurring in the sales of both departments. Just before 2015, and from that point onwards there seems to have been a surge in demand for the 1st hobbies department, and overall the demand has been higher for the 1st department compared to the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlHor7pG_sZg"
   },
   "source": [
    "Now that we've had a look at the departments, let's move on to store level sales.\n",
    "\n",
    "## Store Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk0vuKDG_sZh",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.716412200Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['store_id'])['sales'].sum().sort_values().plot(kind='barh')\n",
    "plt.title('Store Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhB7_cxK_sZh",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.716412200Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(['store_id'])['revenues'].sum().sort_values().plot(kind='barh')\n",
    "plt.title('Store Revenues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52INm8dG_sZh"
   },
   "source": [
    "Looking at revenues, we notice the two stores in California making the top two while two stores in Texas come next. Let's see the sales in the californian stores more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP0sJ5j8_sZh",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.717448Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['store_id'] == 'CA_1']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxzUxPIf_sZi",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.717448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the first store in California\n",
    "\n",
    "df.loc[(df.store_id == 'CA_1')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 1st Store in California')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOBhIdcq_sZi"
   },
   "source": [
    "We can see that the majority of sales have occurred at the end of 2011 to 2014. There is a steady inflow of sales with a few spikes when the sales have been much higher than normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ0XF8ho_sZi",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.717448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "ca1_sales = df[df['store_id'] == 'CA_1'].groupby(['date'])['sales'].sum()\n",
    "ca1_sales_monthly = ca1_sales.resample('M').sum()\n",
    "\n",
    "fig = ca1_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 1st Store in California')\n",
    "fig = seasonal_decompose(ca1_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CrZ2IhA_sZi"
   },
   "source": [
    "By using the statistical technique of time series decomposition we divided the monthly sales into three components that consist of the trend, seasonal and what is left afterwars, the residual. From this we notive an upward trend and a seasonal fluctuation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8HpuiDX_sZj",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.718447600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "def tsplot(y, lags=None):\n",
    "    \"\"\"\n",
    "    Time Series, ACF, PACF, and Augmented Dickey–Fuller Test.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "           \n",
    "    fig = plt.figure()\n",
    "    layout = (2, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "\n",
    "    y.plot(ax=ts_ax)\n",
    "    p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "    ts_ax.set_title('Time Series Analysis \\n P-Value for Dickie-Fuller: {0:.5f}'.format(p_value))\n",
    "    plot_acf(y, lags=lags, ax=acf_ax)\n",
    "    plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "    plt.tight_layout()\n",
    "\n",
    "tsplot(ca1_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjoXj4nX_sZj"
   },
   "source": [
    "A closer look at the series shows an increasing trend  together with a downward spike that occurs frequently, which means that once a year there is zero sales which could indicate the store being closed alltogether. \n",
    "\n",
    "The ACF and the PACF show autocorrelation from the past sales all the way extending to 28 days. The ACF that helps to see how many lag components should be added for the MA components shows that the first 9 days are significantly correlated, as well as the two last days of each week and the first two days of each week. The PACF, on the other hand, that helps to pinpoint the amount of lag for the AR components shows that all the first 9 days, except the third day, are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 1st store appear to have a clear linear postively increasing trend together with seasonal fluctuations that occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLFy4Y4S_sZk",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.718447600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 2nd store in California\n",
    "\n",
    "df[df['store_id'] == 'CA_2']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK7hrvO1_sZk",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.718447600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the second store in California\n",
    "\n",
    "df.loc[(df.store_id == 'CA_2')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 2nd Store in California')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMbocOjg_sZk"
   },
   "source": [
    "Looking at the sales in the second store in California we can see that after the middle of 2015 the sales have moved to a higher level and there are a number of high spikes when the sales have been a lot higher than normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL5XcaUr_sZk",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.719410600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "ca2_sales = df[df['store_id'] == 'CA_2'].groupby(['date'])['sales'].sum()\n",
    "ca2_sales_monthly = ca2_sales.resample('M').sum()\n",
    "\n",
    "fig = ca2_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 2nd Store in California')\n",
    "fig = seasonal_decompose(ca2_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS47ig-H_sZk"
   },
   "source": [
    "There is an increasing trend until 2014, a decreasing trend until 2015 and then after 2015 again an increasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGHi9Vl9_sZl",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.719410600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(ca2_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcpseW5N_sZl"
   },
   "source": [
    "The sales again show a spike downwards indicating zero sales.\n",
    "\n",
    "The ACF shows that the first 9 days are significantly correlated, as well as the last days of each week and the first two days of each week. The PACF shows significance for the first 3 days, and for the 5th, 6th, 7th and 9th days, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 1st store appear to have a clear linear postively increasing trend together with seasonal fluctuations that occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVeVYsxo_sZl",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.720413200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 3rd store in California\n",
    "\n",
    "df[df['store_id'] == 'CA_3']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8aqeJo4_sZl",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.720413200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the third store in California\n",
    "\n",
    "df.loc[(df.store_id == 'CA_3')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 3rd Store in California')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbzv-Dyc_sZm"
   },
   "source": [
    "The sales in the third store is similar to the first store. Again, the bulk of the sales have occurred in 2012 to 2014, and since this period there has been a bit less sales with several high spikes occurring especially from 2011 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RReecpE4_sZm",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.720413200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "ca3_sales = df[df['store_id'] == 'CA_3'].groupby(['date'])['sales'].sum()\n",
    "ca3_sales_monthly = ca3_sales.resample('M').sum()\n",
    "\n",
    "fig = ca3_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 3rd Store in California')\n",
    "fig = seasonal_decompose(ca3_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvgsdqh1_sZm"
   },
   "source": [
    "The monthly sales show an upward trend from 2011 to 2014 where the growth seems to have stalled and even gone downwards from 2016 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qg3vSJh8_sZm",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.721412100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(ca3_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlLB6cEW_sZm"
   },
   "source": [
    "The sales show a clear upward trend with seasonal fluctuations together wtih yearly downward spikes. The ACF shows that the first 9 days are significantly correlated, as well as the last days of each week and the first day of each week. The PACF shows that all the first 9 days, except the second day, are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 1st store appear to have a clear linear postively increasing trend together with seasonal fluctuations that occur frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRNZKfI7_sZn",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.721917300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 4th store in California\n",
    "\n",
    "df[df['store_id'] == 'CA_4']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC2GjAp6_sZn",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.723928500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the fourth store in California\n",
    "\n",
    "df.loc[(df.store_id == 'CA_4')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 4th Store in California')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUL1wkt2_sZn"
   },
   "source": [
    "Similar than what we have seen with the second store, where the bulk of sales have occurred during 2012 to 2014 and after this period, there is a steady inflow of sales with a few occasional high spikes in 2015 and 2016 around at the end of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAT-xOnO_sZn",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.724928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "ca4_sales = df[df['store_id'] == 'CA_4'].groupby(['date'])['sales'].sum()\n",
    "ca4_sales_monthly = ca1_sales.resample('M').sum()\n",
    "\n",
    "fig = ca4_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 4th Store in California')\n",
    "fig = seasonal_decompose(ca4_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayGIA-r7_sZn"
   },
   "source": [
    "From the decomposition we can see a clear linear upward trend from 2012 to 2014, and a seasonal component that occurs frequently in a similar fashion as with the other stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxa7PgGR_sZo",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.724928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(ca4_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWh3P6QH_sZo"
   },
   "source": [
    "The sales show an upward trend and downward spikes at each year as we have seen before.\n",
    "\n",
    "The ACF shows that all the days from the 1st till the 28th are significantly correlated. The PACF shows that all the first 10 days are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 4th store appear to have a clear linear positively increasing trend together with seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sL7_jFnk_sZo",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.724928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 1st store in Texas\n",
    "\n",
    "df[df['store_id'] == 'TX_1']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgcAkyIW_sZo",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.725927900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 1st store in Texas\n",
    "\n",
    "df.loc[(df.store_id == 'TX_1')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 1st Store in Texas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lno6s_cp_sZo",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.725927900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "tx1_sales = df[df['store_id'] == 'TX_1'].groupby(['date'])['sales'].sum()\n",
    "tx1_sales_monthly = tx1_sales.resample('M').sum()\n",
    "\n",
    "fig = tx1_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 1st Store in Texas')\n",
    "fig = seasonal_decompose(tx1_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdDcu_y2_sZp",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.726939300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(tx1_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RG_VIgV1_sZp"
   },
   "source": [
    "The sales in the 1st store in Texas show an upward trend and downward spikes at each year as we have seen with the Californian stores before.\n",
    "\n",
    "The ACF shows that the 1st, 2nd, 5th, 6th and 7th days are correlated as well as the first and last day of each week. The PACF shows  all the first 7 days and the 9th day are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 1st store in Texas appear to have a clear linear positively increasing trend together with seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aU0BYbG_sZp",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.726939300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 2nd store in Texas\n",
    "\n",
    "df[df['store_id'] == 'TX_2']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjyywU9q_sZp",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.726939300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 2nd store in Texas\n",
    "\n",
    "df.loc[(df.store_id == 'TX_2')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 2nd Store in Texas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX6OS75r_sZq",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.727927400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "tx2_sales = df[df['store_id'] == 'TX_2'].groupby(['date'])['sales'].sum()\n",
    "tx2_sales_monthly = tx2_sales.resample('M').sum()\n",
    "\n",
    "fig = tx2_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 2nd Store in Texas')\n",
    "fig = seasonal_decompose(tx2_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3_l_0BK_sZq",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.727927400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(tx2_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fhWvwi2_sZq"
   },
   "source": [
    "The sales in the 2nd store in Texas show a fluctuating upward and downward trend together with downward spikes that happen at each year as we have seen with all the other stores before.\n",
    "\n",
    "The ACF shows that the 1st, 2nd, 5th, 6th, 7th and 8th days are correlated as well as the first and last day of each week. The PACF shows  all the first 9 days are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6K8wAN2_sZq",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.727927400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 3rd store in Texas\n",
    "\n",
    "df[df['store_id'] == 'TX_3']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2ETqFYg_sZr",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.728927600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 3rd store in Texas\n",
    "\n",
    "df.loc[(df.store_id == 'TX_3')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 3rd Store in Texas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnsM9tPA_sZr",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.728927600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "tx3_sales = df[df['store_id'] == 'TX_3'].groupby(['date'])['sales'].sum()\n",
    "tx3_sales_monthly = tx3_sales.resample('M').sum()\n",
    "\n",
    "fig = tx3_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 3rd Store in Texas')\n",
    "fig = seasonal_decompose(tx3_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQY3IywF_sZr",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.728927600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(tx3_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-lRASkA_sZr"
   },
   "source": [
    "The sales in the 3rd store in Texas show an upward trend and downward spikes when the store has been closed.\n",
    "\n",
    "The ACF shows all the past days are correlated. The PACF shows that the 1st, 4th, 5th, 6th and 7th day are, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 3rd store in Texas appear to have a clear linear positively increasing trend together with seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkNRnd7i_sZr",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.729927200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 1st store in Wisconsin\n",
    "\n",
    "df[df['store_id'] == 'WI_1']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wud4F6P8_sZs",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.729927200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 1st store in Wisconsin\n",
    "\n",
    "df.loc[(df.store_id == 'WI_1')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 1st Store in Wisconsin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9IbhvWm_sZs",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.729927200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "wi1_sales = df[df['store_id'] == 'WI_1'].groupby(['date'])['sales'].sum()\n",
    "wi1_sales_monthly = wi1_sales.resample('M').sum()\n",
    "\n",
    "fig = wi1_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 1st Store in Wisconsin')\n",
    "fig = seasonal_decompose(wi1_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8_yVU3W_sZs",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.729927200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(wi1_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wElKHd8c_sZs"
   },
   "source": [
    "The sales in the 1st store in Wisconsin show an upward trend and a structural change that happened at the end of 2012 where the store perhaps received an upgrade to help reach bigger sales. The downward spikes happen again at each year as we have seen with the other stores before.\n",
    "\n",
    "The ACF shows that tall the days are correlated. The PACF shows  all the first 6 days and the 9th day are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 1st store in Wisconsin appear to have a clear linear positively increasing trend together with seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGG529VJ_sZs",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.729927200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 2nd store in Wisconsin\n",
    "\n",
    "df[df['store_id'] == 'WI_2']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCGGKD_k_sZt",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.731483200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 2nd store in Wisconsin\n",
    "\n",
    "df.loc[(df.store_id == 'WI_2')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 2nd Store in Wisconsin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzuwO8ic_sZt",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.731483200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "wi2_sales = df[df['store_id'] == 'WI_2'].groupby(['date'])['sales'].sum()\n",
    "wi2_sales_monthly = wi2_sales.resample('M').sum()\n",
    "\n",
    "fig = wi2_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 2nd Store in Wisconsin')\n",
    "fig = seasonal_decompose(wi2_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFGcAh4l_sZt",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.731483200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(wi2_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdFqN9qY_sZu"
   },
   "source": [
    "The sales in the 2nd store in Wisconsin show an upward trend and a structural change at the beginning of 2012. The downward spikes at each year as we have seen with the other stores before.\n",
    "\n",
    "The ACF shows that all the days are correlated. The PACF shows  all the first 15 days are significantly correlated, as well as the last day of each week and the first day of each week.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity, and conclude that the sales from the 2nd store in Texas appear to have a clear linear positively increasing trend together with seasonal fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4MtzlY9_sZu",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.732437200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Statistics of sales from 3rd store in Wisconsin\n",
    "\n",
    "df[df['store_id'] == 'WI_3']['sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdmbIIa4_sZu",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.732437200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting sales in the 3rd store in Wisconsin\n",
    "\n",
    "df.loc[(df.store_id == 'WI_3')][['date', 'sales']].set_index('date').plot()\n",
    "plt.title('Sales in the 3rd Store in Wisonsin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjiNuGHi_sZv",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.733436800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Monthly Sales and Decomposition of Time Series\n",
    "\n",
    "wi3_sales = df[df['store_id'] == 'WI_3'].groupby(['date'])['sales'].sum()\n",
    "wi3_sales_monthly = wi3_sales.resample('M').sum()\n",
    "\n",
    "fig = wi3_sales_monthly.plot(kind='line', ax=plt.subplot(1,1,1))\n",
    "plt.title('Monthly Sales in the 3rd Store in Wisconsin')\n",
    "fig = seasonal_decompose(wi3_sales_monthly, model='multiplicative', period=12).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_4-pw0Z_sZ0",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.733436800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "tsplot(wi3_sales.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EndqCSku_sZ0"
   },
   "source": [
    "The sales in the 3rd store in Wisconsin show a fluctuating upwardand downward trend with the downward spikes as before.\n",
    "\n",
    "The ACF shows that the first 2 weeks are correlated as well as the first and last day of each week. The PACF shows  all the first 7 days and the 9th day are significantly correlated, as well as the last day of each week and the first day of each week. The correlation seems to reach higher levels at the end of the month, especially with the ACF.\n",
    "\n",
    "The Dickie-Fuller test confirms the results so that we cannot reject the null hypothesis of non-stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Ta72_w_sZ0"
   },
   "source": [
    "## Correlation\n",
    "\n",
    "Now that we've covered where the sales have occurred let's have a closer look at the correlation between the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd8AHhi2_sZ0",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.734436900Z"
    }
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIGnY4UM_sZ0",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.734436900Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ujjuAEz_sZ1"
   },
   "source": [
    "We see a strong correlation between the target sales and revenues which is normal, but we can't use the revenues to determine future sales because we would already know how much the revenues are changing in the future so we need to remove the revenues from modeling. The year is also strongly correlated with the variable 'wm_yr_wk', and as we don't need two variables indicating the same thing so we will remove the year from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2_1GJ-K_sZ1",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.734436900Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['revenues', 'year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrTVlmXN_sZ1"
   },
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "In the beginning we identified 5 different components for Time Series forecasting which consisted of:\n",
    "\n",
    "- `Autoregressive (AR)` components in the form of lags from past autocorrelation of the target variable. We could see that there was strong autocorrelation from the past days of sales so we will introduce lags of days that will cover the 28 days as inline with the forecasting period. More specifucally, the PACF showed statistically significant autocorrelation for the 9 first days, and for the last and first day of each week so these will be our AR variables. \n",
    "\n",
    "\n",
    "- `Moving Average (MA)` components in the form of lags that are calculated from the averages of variables in the past. This is similar to the autoregressive components, but the lag terms are smoothed out by taking the mean of the change during a certain window of days.The ACF showed statistically significant partial autocorrelation for the 9 first days, and for the last and first day of each week , so we will use these as our MA variables.\n",
    "\n",
    "\n",
    "- `Trend` components, in the form of linear or non-linear trend in product sales. We could see some indications of a linear trend in some of the years, especially while looking closer at the particular stores located in the different States. By comparing the sales to the daily average sales we can compute the trend component for each store separetely. \n",
    "\n",
    "\n",
    "- `Seasonal` components, that occur on a certain particular time with a certain frequency. By decompositing the monthly sales we could see a wave-like pattern occurring during the year, and by looking the sales figures at each month, we could notice that most of the sales occur during the 1st quarter of the year. We already have the calendar with all the dates of the sales as well as the special events occurring in the US marked down, but we will also create some additional variables that will capture seasonality with especially highs and lows in occurred sales.\n",
    "\n",
    "\n",
    "- `Exogenous` variables, that come outside the model. By comparing the daily price information with the prices of different products in the same product category we are able to capture the competition between the different trade marks and companies that set price changes that affect consumer demand.\n",
    "\n",
    "We will start by creating these components one-by-one and finish off with encoding all the categorical features by mapping them with category codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw8WM8qz_sZ1"
   },
   "source": [
    "## Relative Prices\n",
    "\n",
    "The prices are set according to market demand and because the stores are of different sizes and located in different States, they will face a different demand for the products, and hence, should have different prices. This allows us to compare the prices and their changes with relative prices:\n",
    "\n",
    "    - by comparing the price changes of a product,\n",
    "    \n",
    "    - by comparing the prices of a product relative to the different stores, and\n",
    "    \n",
    "    - by comparing the prices of similar products in the same store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VwccvNU_sZ1",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.735436700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create relative prices\n",
    "\n",
    "def relative_prices(df):\n",
    "    \"\"\"Creating relative prices from the prices of the products sold in different stores.\n",
    "    Relative prices are calculated by comparing the same products in other stores,\n",
    "    differences in prices during time and other similar products in the store.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate Average price for all stores\n",
    "    df_mean_store = pd.DataFrame(df.groupby(['item_id', 'wm_yr_wk'])['sell_price'].mean())\n",
    "    df_mean_store.columns = ['item_sells_price_avg']\n",
    "    df_mean_store.reset_index(inplace = True)\n",
    "\n",
    "    # Combine with calendar\n",
    "    prices_new = pd.merge(df, df_mean_store, on=['item_id', 'wm_yr_wk'], how='left', suffixes=('', '_y'))\n",
    "    prices_new.drop(prices_new.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "    # Price difference with the same products in other stores\n",
    "    prices_new['delta_price_all_rel'] = (prices_new['sell_price'] - \n",
    "                                         prices_new['item_sells_price_avg'])/prices_new['item_sells_price_avg']\n",
    "\n",
    "    # Price difference with last week\n",
    "    prices_new['item_store'] = prices_new['item_id'].astype(str) + '_' + prices_new['store_id'].astype(str)\n",
    "    prices_new['item_store_change'] = prices_new[\"item_store\"].shift() != prices_new[\"item_store\"]\n",
    "    # Price difference week n - week n-1\n",
    "    prices_new['delta_price_weekn-1'] = (prices_new['sell_price']-\n",
    "                                         prices_new['sell_price'].shift(1)).fillna(0)/prices_new['sell_price'].shift(1)\n",
    "    prices_new['delta_price_weekn-1'] = prices_new['delta_price_weekn-1'].fillna(0) * (prices_new['item_store_change']==0)\n",
    "\n",
    "    # Average price of the department by store\n",
    "    prices_new['dept_id'] = prices_new.item_id.str[:-4]\n",
    "    df_mean_cat = pd.DataFrame(prices_new.groupby(['dept_id', 'store_id', 'wm_yr_wk'])['sell_price'].mean())\n",
    "    df_mean_cat.columns = ['dept_sells_price_avg']\n",
    "    df_mean_cat.reset_index(inplace = True)\n",
    "    # Combine with price dataset\n",
    "    prices_new = pd.merge(prices_new, df_mean_cat, on=['dept_id', 'store_id', 'wm_yr_wk']\n",
    "                          , how='left', suffixes=('', '_y'))\n",
    "    prices_new.drop(prices_new.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)\n",
    "\n",
    "    # Compare the product price with the average of the department (category)\n",
    "    prices_new['delta_price_cat_rel'] = (prices_new['sell_price'] - \n",
    "                                         prices_new['dept_sells_price_avg'])/prices_new['dept_sells_price_avg']                                               \n",
    "\n",
    "    # Drop columns\n",
    "    prices_new.drop(['item_sells_price_avg', 'item_store_change', 'item_store_change', 'item_store',\n",
    "                    'dept_sells_price_avg'], axis = 1, inplace = True)\n",
    "    \n",
    "    return prices_new\n",
    "\n",
    "df = relative_prices(df)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4_w-5EJ_sZ2"
   },
   "source": [
    "## Autoregressive Components\n",
    "\n",
    "Based on the PACF graph, we will create lag terms of 1 to 9 days, and include the last day and first day of each week. This way we will have 14 autoregressive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldKtGQ6P_sZ2",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.735436700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lag terms for 1, 2, 3, 7, 14, 21 and 28 days\n",
    "\n",
    "lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 20, 21, 28]\n",
    "for lag in lags:\n",
    "    df['sales_lag_'+str(lag)] = df.groupby(\n",
    "        ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "        as_index = False)['sales'].shift(lag).astype(np.float16)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKrDF7IE_sZ2"
   },
   "source": [
    "## Moving Average Components\n",
    "\n",
    "We will create different components based on averages. First,  we will use averages to describe and differentiate the charasterics of different products, states, stores, categories and departments. Then, we will create additional group categories by combining two or three of these variables together that could give us some additional insight related to the sales of the products.\n",
    "\n",
    "<h3> Averages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sSwLDJI_sZ3",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.735436700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total Average Sales by item, state, store, cat and dept\n",
    "\n",
    "df['item_sales_avg'] = df.groupby('item_id')['sales'].transform('mean').astype(np.float16)\n",
    "df['state_sales_avg'] = df.groupby('state_id')['sales'].transform('mean').astype(np.float16)\n",
    "df['store_sales_avg'] = df.groupby('store_id')['sales'].transform('mean').astype(np.float16)\n",
    "df['cat_sales_avg'] = df.groupby('cat_id')['sales'].transform('mean').astype(np.float16)\n",
    "df['dept_sales_avg'] = df.groupby('dept_id')['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Sales average by group\n",
    "\n",
    "df['cat_dept_sales_avg'] = df.groupby(['cat_id','dept_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['store_item_sales_avg'] = df.groupby(['store_id','item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['cat_item_sales_avg'] = df.groupby(['cat_id','item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['dept_item_sales_avg'] = df.groupby(['dept_id','item_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['state_store_sales_avg'] = df.groupby(['state_id','store_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['state_store_cat_sales_avg'] = df.groupby(['state_id','store_id','cat_id'])['sales'].transform('mean').astype(np.float16)\n",
    "df['store_cat_dept_sales_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n75i-16U_sZ3"
   },
   "source": [
    "<h3> Rolling Averages</h3>\n",
    "\n",
    "Next, we will calculate the moving averages based on sales and also calculate the average from the lag terms up to 28 days. We include the amount of lag suggested by the ACF plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3J7eK4S_sZ3",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.736438100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rolling Average for the last n days\n",
    "\n",
    "for days in [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 21, 28]:\n",
    "    df['rolling_sales_mean_{}'.format(days)] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform(\n",
    "    lambda x: x.rolling(window=days).mean()).astype(np.float16)\n",
    "\n",
    "# Rolling Average on actual lag\n",
    "\n",
    "for window, lag in zip([7, 7, 28, 28], [7, 28, 7, 28]):\n",
    "    df['rolling_lag_{}_win_{}'.format(window, lag)] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales_lag_{}'.format(lag)].transform(\n",
    "    lambda x: x.rolling(window=window).mean()).astype(np.float16)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oit3fUjR_sZ4"
   },
   "source": [
    "## Trend Components\n",
    "\n",
    "When we analyzed the sales in the different stores previously, we noticed that there were differences in the trends when looking at the state, store and department levels, but also with different categories and products, so we need to take this into account.\n",
    "\n",
    "First, we will calculate the trend from the daily average and the average of sold products regarding all stores in all states, and then the item selling trend from the daily average and the average of sold products regarding all categories in all departments. This way we will have two trend variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfpideRF_sZ4",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.736438100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selling trends\n",
    "\n",
    "# Daily Average\n",
    "df['daily_avg_sales'] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Total Average \n",
    "df['avg_sales'] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Selling Trend\n",
    "df['selling_trend'] = (df['daily_avg_sales'] - df['avg_sales']).astype(np.float16)\n",
    "\n",
    "# Daily Average \n",
    "df['item_daily_avg_sales'] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'd'])['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Total Average \n",
    "df['item_avg_sales'] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id'])['sales'].transform('mean').astype(np.float16)\n",
    "\n",
    "# Selling Trend\n",
    "df['item_selling_trend'] = (df['item_daily_avg_sales'] - df['item_avg_sales']).astype(np.float16)\n",
    "\n",
    "# Drop Columns \n",
    "df.drop(['daily_avg_sales','avg_sales','item_daily_avg_sales','item_avg_sales'],axis=1,inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SqSfYPY_sZ4"
   },
   "source": [
    "## Seasonal Components\n",
    "\n",
    "When we had a closer look at some particular products that had been sold during the 5.5 year period, we noticed that most of the times the products where not being sold at all, but overall there was a steady demand for products throughout the years with the occasional high spikes occurring on a particular day or period.\n",
    "\n",
    "These events which occur on a certain particular time with a certain frequency can be modeled by looking at the days when a high or low amount of products were sold. As the sales are in count data form, we can look at the rolling maximimum and minimum value during a certain period of days. We will include the lag terms so that we try to catch all the days of the week and then add 2, 3 and 4 weeks inorder to catch seasonal patterns in the sales. In total we will have 24 seasonal variables added to the event variables we already had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EV1Wmlyr_sZ4",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.736438100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rolling Maximum for the last n days\n",
    "\n",
    "for days in [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 21, 28]:\n",
    "    df['rolling_sales_max_{}'.format(days)] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform(\n",
    "    lambda x: x.rolling(window=days).max()).astype(np.float16)\n",
    "\n",
    "# Rolling Minimum for the last n days\n",
    "\n",
    "for days in [1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 21, 28]:\n",
    "    df['rolling_sales_min_{}'.format(days)] = df.groupby(\n",
    "    ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sales'].transform(\n",
    "    lambda x: x.rolling(window=days).min()).astype(np.float16)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c75RGBXg_sZ5"
   },
   "source": [
    "## Mapping Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epCvlSX-_sZ5",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.737438300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping categories\n",
    "\n",
    "# dtypes to categories\n",
    "df.store_id = df.store_id.astype('category')\n",
    "df.item_id = df.item_id.astype('category')\n",
    "df.cat_id = df.cat_id.astype('category')\n",
    "df.state_id = df.state_id.astype('category')\n",
    "df.id = df.id.astype('category')\n",
    "df.dept_id = df.dept_id.astype('category')\n",
    "\n",
    "# dictionary of categories\n",
    "d_id = dict(zip(df.id.cat.codes, df.id))\n",
    "d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\n",
    "d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\n",
    "d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\n",
    "d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\n",
    "d_state_id = dict(zip(df.state_id.cat.codes, df.state_id))\n",
    "\n",
    "# Save to DataFrame\n",
    "LIST_SAVE = [d_id, d_item_id, d_dept_id, d_cat_id, d_store_id, d_state_id]\n",
    "LIST_NAME = ['d_id', 'd_item_id', 'd_dept_id', 'd_cat_id', 'd_store_id', 'd_state_id']\n",
    "for list_save, list_name in zip(LIST_SAVE, LIST_NAME):\n",
    "    pickle.dump(list_save, open('data/sales_forecasting/{}.p'.format(list_name), \"wb\"))  \n",
    "\n",
    "# Remove d_ and transform to int (dates)\n",
    "df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n",
    "cols = df.dtypes.index.tolist()\n",
    "types = df.dtypes.values.tolist()\n",
    "\n",
    "# Transform categorical data to codes\n",
    "for i,type in enumerate(types):\n",
    "    if type.name == 'category':\n",
    "        df[cols[i]] = df[cols[i]].cat.codes\n",
    "        \n",
    "# Drop Dates\n",
    "df.drop('date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SusoP6XL_sZ5",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.737438300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving dataset\n",
    "\n",
    "df.to_pickle('data/sales_forecasting/data_features_improved.pkl')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZRJyOGA_sZ5",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.737438300Z"
    }
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meUXRJ8B_sZ5"
   },
   "source": [
    "# 5. Modeling\n",
    "\n",
    "\n",
    "## The Model\n",
    "\n",
    "We will be using the Light Gradient Boosting Machine (LGBM Regressor) which is a decision tree based emsemble technique, where the novelty not only lies, in using bagging and boosting, but also in the histogram-way of bucketing the values, which leads to less memory usage, and makes the model faster to train compared to other similar techniques. \n",
    "\n",
    "Bagging, enables the model to fit many models to bootstrapped samples of the data and average over the models. In addition, the model will sample the predictor variables when splitting the trees in similar fashion as with the Random Forest algorithm. Boosting, on the other hand, enables to fit a sequence of models by giving more weight to the records with large residuals for each successive rounds. This means that each successive model seeks to minimize the error, or in other words, attempts to fix the error by giving more weight to the biggest mistakes that were made by the previous model.\n",
    "\n",
    "In order to succeed well, several hyperparameters need to be set before fitting the model. These adjust the structure and the accuracy of the model, and control for possible overfitting issues.\n",
    "\n",
    "### Hyperparameters \n",
    "\n",
    "\n",
    "<h4 align='center'> Model Structure </h4>\n",
    "\n",
    "\n",
    "        - `numleaves`:               Controls the amount of leaves at the node of the tree where the decision happens. \n",
    "    \n",
    "        - `max_depth`:               Controls the depthness or the complexity in the decision-making process. \n",
    "        \n",
    "<h4 align='center'> Model Accuracy </h4>\n",
    "\n",
    "\n",
    "        - `n_estimators`:            Controls the amount of trees being used.\n",
    "    \n",
    "        - `learning_rate`:           Step size of the gradient descent that helps to reach the optimum values.\n",
    "    \n",
    "        - `objective`:               Poisson regression.\n",
    " \n",
    "<h4 align='center'> Overfitting </h4>\n",
    "\n",
    "\n",
    "        - `reg_lamda`:               Ridge regularization (L2) adds a penalty term to parameters of the cost function. \n",
    "    \n",
    "        - `min_child_weight`:        The minimum amount of training samples at each node.\n",
    "    \n",
    "        - `subsample`:               Fraction of the training samples that are randomly selected to train each tree.\n",
    "    \n",
    "        - `colsample_bytree`:        Fraction of the features that are randomly selected to train each tree.\n",
    "    \n",
    "        - `early_stopping_rounds`:   Stop training when the model is not improving anymore.\n",
    "\n",
    "### Training the Model\n",
    "\n",
    "We will train one model for each store, and the variables will be trained in different steps, where we will be adding a new component at each step, and then see which of the components were important in the decision-making of the forecast. \n",
    "\n",
    "For the variables, we will start from the 56th day, since that's when the lag terms have started to calculate values from the previous events. This will help to speed-up the training and hopefully reach a better accuracy with better quality data.\n",
    "\n",
    "For the training, we will use the trees to act as Random Forests so that each tree will select a random sample of the training samples and a random subset of the features for each training session. Then, with the help of boosting, each tree will build upon the forecast of the previous tree and the model will stop training once the forecast haven't been improving for some time. \n",
    "\n",
    "### Evaluating the Model\n",
    "\n",
    "We will be evaluating the different steps that add new features at each step based on the fifferent components that make the forecast of time series possible. The metric for calculating the error during training will be the Root Mean Squared Error (RMSE) which is the root of the average of squared difference between the actual and predicted values.\n",
    "\n",
    "$$ {RMSE} = \\sqrt{\\frac{1}{n}\\sum_{t=1}^{n} (y_t - \\hat{y}_t)^2} $$\n",
    "\n",
    "There are a number of ways to calculate the error regarding the forecast, but the RMSE has a tendance to put a larger weight on the outliers of the samples in order to progress towards the mean, and thus, should help the model to deal with the high-spikes that were particularly seen in the graphs of the store sales earlier.\n",
    "\n",
    "After training the model we will compute the errors with the Mean Absolute Error (MAE) which is the mean absolute difference between the actual and predicted values.\n",
    "\n",
    "$$ {MAE} = {\\frac{1}{n}\\sum_{t=1}^{n} |y_t - \\hat{y}_t|} $$\n",
    "\n",
    "Finally, LGBM - being a tree-based model -, enables us to also see which of the features were more important than others at each step when the trees made their decisions, so we will be analyzing all the results, before the fine-tuning of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXE2PfVB_sZ6"
   },
   "source": [
    "## Preparing the Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4pjknv8_sZ6",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.738438600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing processed dataset\n",
    "\n",
    "data = pd.read_pickle('data/sales_forecasting/data_features_improved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVGE7J0i_sZ6",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.738438600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing training sequence\n",
    "\n",
    "# Dictionary of id categories\n",
    "LIST_NAME = ['d_id', 'd_item_id', 'd_dept_id', 'd_cat_id', 'd_store_id', 'd_state_id']\n",
    "dict_data = {}\n",
    "for list_name in LIST_NAME:\n",
    "    dict_temp = pickle.load(open('data/sales_forecasting/{}.p'.format(list_name), \"rb\"))\n",
    "    dict_data[list_name] = dict_temp\n",
    "    del dict_temp\n",
    "\n",
    "# Initial Features\n",
    "INIT = list(data.columns[0:20])\n",
    "\n",
    "# Auto-Regressive Components\n",
    "AR = list(data.columns[23:37])\n",
    "\n",
    "# Moving-Average Components\n",
    "MA = list(data.columns[37:65])\n",
    "\n",
    "# Trend Components\n",
    "TREND = list(data.columns[65:67])\n",
    "\n",
    "# Seasonal Components\n",
    "SEASONAL = list(data.columns[67:91])\n",
    "\n",
    "# Relative Prices\n",
    "RELATIVE_PRICES = list(data.columns[20:23])\n",
    "\n",
    "# Dictionary of features\n",
    "dict_features = {\n",
    "    'STEP_1': INIT,\n",
    "    'STEP_2': INIT+AR,\n",
    "    'STEP_3': INIT+AR+MA,\n",
    "    'STEP_4': INIT+AR+MA+TREND,\n",
    "    'STEP_5': INIT+AR+MA+TREND+SEASONAL,\n",
    "    'STEP_6': INIT+AR+MA+TREND+SEASONAL+RELATIVE_PRICES,\n",
    "}\n",
    "\n",
    "# List of step names\n",
    "LIST_STEPS = ['STEP_1', 'STEP_2', 'STEP_3', 'STEP_4', 'STEP_5', 'STEP_6']\n",
    "\n",
    "# List of sequence names\n",
    "SEQUENCE = ['INIT', 'INIT + AR', \n",
    "                  'INIT + AR + MA',\n",
    "                  'INIT + AR + MA + TREND',\n",
    "                  'INIT + AR + MA + TREND + SEASONAL',\n",
    "                  'INIT + AR + MA + TREND + SEASONAL + RELATIVE PRICES']\n",
    "\n",
    "# Dictionary of sequences\n",
    "dict_sequence = dict(zip(LIST_STEPS, SEQUENCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvWuCTDr_sZ6"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjos6Y5b_sZ7",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.738438600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training model for each store individually\n",
    "\n",
    "# Store names and ids\n",
    "stores = data.store_id.unique()\n",
    "d_store_id = dict_data['d_store_id']\n",
    "\n",
    "# Dictionary for errors\n",
    "dict_error_valid = {} \n",
    "\n",
    "\n",
    "for step in LIST_STEPS: # Loop with different steps\n",
    "\n",
    "    print('SALES FORECAST: {}'.format(dict_sequence[step]))\n",
    "    print('=' * 67)\n",
    "    \n",
    "    # Folder to save\n",
    "    FOLDER_MODEL = 'data/sales_forecasting/{}/'.format(step)\n",
    "    Path(FOLDER_MODEL).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    COLS_SCOPE = dict_features[step] # Filter to select sequence for step\n",
    "    data_scope = data[COLS_SCOPE].copy() # Select data with filter\n",
    "\n",
    "    # Validation set for all stores\n",
    "    valid = data_scope[(data_scope['d']>=1914) & (data_scope['d']<1942)][['id','d','sales']]\n",
    "\n",
    "    # Target values\n",
    "    valid_set = valid['sales']\n",
    "\n",
    "    # Dataframe for predictions\n",
    "    df_validpred = pd.DataFrame()\n",
    "\n",
    "    for store in stores: # Looping the model by training for each store separately\n",
    "\n",
    "        df = data_scope[data_scope['store_id']==store] # Dataframe for each store\n",
    "\n",
    "        X_train, y_train = df[df['d']<1914].drop('sales',axis=1), df[df['d']<1914]['sales'] # All except the last 28 days\n",
    "\n",
    "        # Validation for 28 days\n",
    "        X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sales',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sales']\n",
    "    \n",
    "        \n",
    "        # Hyperparameters \n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.08,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            max_depth=8,\n",
    "            num_leaves=100,\n",
    "            min_child_weight=300,\n",
    "            objective='poisson',\n",
    "            reg_lambda=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "        # Training with validation set and early stopping\n",
    "        model.fit(X_train, y_train, \n",
    "                  eval_set = [(X_train,y_train),(X_valid,y_valid)], \n",
    "                  eval_metric = 'rmse', \n",
    "                  verbose = 0, \n",
    "                  early_stopping_rounds = 20)\n",
    "\n",
    "        # Validation prediction\n",
    "        valid_pred = model.predict(X_valid)\n",
    "\n",
    "        # DataFrame for all predictions\n",
    "        df_valid = pd.DataFrame({\n",
    "            'validation':valid_set[X_valid.index],\n",
    "            'valid_prediction':valid_pred,\n",
    "            'store': d_store_id[store]\n",
    "        })\n",
    "        df_valid['valid_error'] = df_valid['validation'] - df_valid['valid_prediction']\n",
    "        \n",
    "        df_validpred = pd.concat([df_validpred, df_valid])\n",
    "\n",
    "        # Save the model\n",
    "        filename = FOLDER_MODEL + 'LGBM_model_' + str(d_store_id[store])+'.pkl'\n",
    "        joblib.dump(model, filename)\n",
    "\n",
    "        del model, X_train, y_train, X_valid, y_valid\n",
    "\n",
    "    # Save prediction for all stores\n",
    "    df_validpred.to_csv(FOLDER_MODEL + 'forecast.csv')\n",
    "\n",
    "    # Validation RMSE\n",
    "    valid_rmse = np.sqrt(np.mean((df_validpred.validation.values - df_validpred.valid_prediction.values) ** 2))/np.mean(df_validpred.validation.values)\n",
    "    \n",
    "    # Add RMSE in a Dictionary\n",
    "    dict_error_valid[step] = valid_rmse\n",
    "    \n",
    "    print(\"VALID RMSE = %0.5f \" % valid_rmse)\n",
    "    print(\"-----\" * 8)\n",
    "\n",
    "# DataFrame for RMSE\n",
    "df_error = pd.DataFrame({\n",
    "    'STEP': LIST_STEPS,\n",
    "    'SEQUENCE': [dict_sequence[step] for step in LIST_STEPS],\n",
    "    'VALID_RMSE': [dict_error_valid[step] for step in LIST_STEPS]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0X0VCOS_sZ7"
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "To see how the model performed, we will plot the error rate with the forecast and see how well the model managed to forecast the sales for all the stores during the 28 day period. We will start by first plotting the RMSE from the different steps, and then look at the actual forecast in details by analyzing the errors at each store separately. Finally, at the end we will see which features contributed to the decision-making the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNWMOLdw_sZ7",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.739438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "\n",
    "sns.barplot(data=df_error, x='VALID_RMSE', y='STEP')\n",
    "plt.title('RMSE for Sales Forecast')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQbTmFXd_sZ7"
   },
   "source": [
    "### Sales Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOvHsdjG_sZ8",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.739438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting forecast for sales total in each store\n",
    "\n",
    "total_store_sales = df_validpred.groupby(['store'])['validation'].sum()\n",
    "predicted_store_sales = df_validpred.groupby(['store'])['valid_prediction'].sum()\n",
    "\n",
    "plt.plot(total_store_sales, marker = 'o', linestyle= '--', color='blue')\n",
    "plt.plot(predicted_store_sales, marker = 'x', linestyle = '--', color='red')\n",
    "plt.title('Total Sales and Sales Forecast in Stores')\n",
    "plt.show()\n",
    "print('Total Sales in Stores    : {}'.format(df_validpred['validation'].sum()))\n",
    "print('Total Sales Predictions  : {}'.format(df_validpred['valid_prediction'].round().sum()))\n",
    "print('Total Sales Error        : {}'.format(df_validpred['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIhg8nkc_sZ8"
   },
   "source": [
    "The model seemed to do quite a good job in forecasting the future sales that predicted only 27 sales wrong on average from the actual total of 1,231,764 sales in all of the stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bklhoTff_sZ8"
   },
   "source": [
    "### Validation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9MZ6ijl_sZ9",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.739438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error\n",
    "\n",
    "df_validpred['valid_error'].plot()\n",
    "plt.title('Validation Error for Stores')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNYT6W6y_sZ9",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.739438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation error by store\n",
    "\n",
    "df_validpred.groupby('store')['valid_error'].plot(legend=True)\n",
    "plt.title('Validation Error in Stores')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQVAc3vM_sZ9"
   },
   "source": [
    "A closer look at the validation error in the stores shows that the three biggest spikes in errors occurred with the 2nd store in California where the model predicted for two days a bigger amount of sales that actually occurred and for one day less sales that what occurred. Otherwise, the error rate seem to follow quite closely the mean of zero. Let's now have a closer look at each store separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIhZ1p3J_sZ9",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.740437500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 1st store in California\n",
    "\n",
    "plt.title('Validation Error for 1st Store in California')\n",
    "df_validpred[df_validpred['store'] == 'CA_1']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'CA_1']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'CA_1']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'CA_1']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCLCekxB_sZ-",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.740437500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 2nd store in California\n",
    "\n",
    "plt.title('Validation Error for 2nd Store in California')\n",
    "df_validpred[df_validpred['store'] == 'CA_2']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :  {}'.format(df_validpred[df_validpred['store'] == 'CA_2']['validation'].sum()))\n",
    "print('Total Prediction  :  {}'.format(df_validpred[df_validpred['store'] == 'CA_2']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'CA_2']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pgzGVp-_sZ-",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.741438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 3rd store in California\n",
    "\n",
    "plt.title('Validation Error for 3rd Store in California')\n",
    "df_validpred[df_validpred['store'] == 'CA_3']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'CA_3']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'CA_3']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'CA_3']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRfPu0Ce_sZ-",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.741438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 4th store in California\n",
    "\n",
    "plt.title('Validation Error for 4th Store in California')\n",
    "df_validpred[df_validpred['store'] == 'CA_4']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'CA_4']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'CA_4']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'CA_4']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCK7EP-8_sZ_",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.741438800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 1st store in Texas\n",
    "\n",
    "plt.title('Validation Error for 1st Store in Texas')\n",
    "df_validpred[df_validpred['store'] == 'TX_1']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :  {}'.format(df_validpred[df_validpred['store'] == 'TX_1']['validation'].sum()))\n",
    "print('Total Prediction  :  {}'.format(df_validpred[df_validpred['store'] == 'TX_1']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'TX_1']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2z_rqiq_sZ_",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.742526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 2nd store in Texas\n",
    "\n",
    "plt.title('Validation Error for 2nd Store in Texas')\n",
    "df_validpred[df_validpred['store'] == 'TX_2']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'TX_2']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'TX_2']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'TX_2']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEpvrEW-_sZ_",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.742526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 3rd store in Texas\n",
    "\n",
    "plt.title('Validation Error for 3rd Store in Texas')\n",
    "df_validpred[df_validpred['store'] == 'TX_3']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'TX_3']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'TX_3']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'TX_3']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6FzuPJo_sZ_",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.742526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 1st store in Wisconsin\n",
    "\n",
    "plt.title('Validation Error for 1st Store in Wisconsin')\n",
    "df_validpred[df_validpred['store'] == 'WI_1']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :  {}'.format(df_validpred[df_validpred['store'] == 'WI_1']['validation'].sum()))\n",
    "print('Total Prediction  :  {}'.format(df_validpred[df_validpred['store'] == 'WI_1']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'WI_1']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lG4d6QBa_saA",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.742526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 2nd store in Wisconsin\n",
    "\n",
    "plt.title('Validation Error for 2nd Store in Wisconsin')\n",
    "df_validpred[df_validpred['store'] == 'WI_2']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :  {}'.format(df_validpred[df_validpred['store'] == 'WI_2']['validation'].sum()))\n",
    "print('Total Prediction  :  {}'.format(df_validpred[df_validpred['store'] == 'WI_2']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'WI_2']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LsuMTiK_saA",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.743526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error for 3nd store in Wisconsin\n",
    "\n",
    "plt.title('Validation Error for 3nd Store in Wisconsin')\n",
    "df_validpred[df_validpred['store'] == 'WI_3']['valid_error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_validpred[df_validpred['store'] == 'WI_3']['validation'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_validpred[df_validpred['store'] == 'WI_3']['valid_prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_validpred[df_validpred['store'] == 'WI_3']['valid_error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPMIwSHq_saA"
   },
   "source": [
    "### Analyzing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeY0Af55_saA",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.743526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "\n",
    "for step in LIST_STEPS:\n",
    "\n",
    "    FOLDER_MODEL = 'data/sales_forecasting/{}/'.format(step)\n",
    "    Path(FOLDER_MODEL).mkdir(parents=True, exist_ok=True)\n",
    "    COLS_SCOPE = dict_features[step]\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    features = [f for f in COLS_SCOPE if f != 'sales']\n",
    "\n",
    "    for store in stores:\n",
    "        store_name = d_store_id[store]\n",
    "        filename = FOLDER_MODEL + 'LGBM_model_' + str(d_store_id[store])+'.pkl'\n",
    "        \n",
    "        model = joblib.load(filename)\n",
    "\n",
    "        # Create feature importances\n",
    "        store_importance_df = pd.DataFrame()\n",
    "        store_importance_df[\"feature\"] = features\n",
    "        store_importance_df[\"importance\"] = model.feature_importances_\n",
    "        store_importance_df[\"store\"] = store_name\n",
    "\n",
    "        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n",
    "\n",
    "    # Calculate feature importances\n",
    "    df_fi_mean = pd.DataFrame(feature_importance_df[\n",
    "        [\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "        by=\"importance\", ascending=False))\n",
    "    df_fi_mean.columns = ['importance']\n",
    "    df_fi_mean['%_importance'] = (100 * df_fi_mean['importance']/df_fi_mean['importance'].sum(axis=0)).round(2)\n",
    "    df_fi_mean.to_csv(FOLDER_MODEL + 'features_{}.csv'.format(step))\n",
    "\n",
    "\n",
    "    # Plot results for top 20 features\n",
    "    cols = df_fi_mean[:20].index\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "    fig = plt.figure(figsize=(6,10))\n",
    "    ax = fig.gca()\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data = best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Top 20 Features: {}'.format(step))\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(FOLDER_MODEL + 'features_{}.png'.format(step))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAEhprpw_saB"
   },
   "source": [
    "## Analyzing the Results\n",
    "\n",
    "From the results we see that at each step, the error term, RMSE, decreased as the model managed to reach higher accuracy compared to the previous steps. With a total a total sales forecast for 1.23M products, the model was able to forecast almost all the future sales with a very high accurary that only missed by 27 sales of the actual total sales on average for each store.\n",
    "\n",
    "A closer look at the stores revealed, that the biggest spike of errors occurred with the 2nd Store and 3rd store in California, and to some extent with the 2nd store in Texas and Wisconsin.\n",
    "\n",
    "In the SARIMAX type of linear models, the residuals are often examined to see if the fit can be improved. However, with boosting, the LGBM is able to fit a series of models in which each successive model seeks to minize the error of the previous model. This leads to the very low residual error that we just saw. We can also see that the residuals look like white noise in that the residuals contain no further information about the dependencies in the data. Moreover, the residuals have a zero mean, the same variance and are not correlated in time.\n",
    "\n",
    "<b> Initial Data </b>\n",
    "\n",
    "Here we had 20 variables that were given to us in the beginning. The model reached an RMSE of 1.672 which can be seen as a baseline for which to compare the results afterwards. The most important features were two variables related to the id of the product, the selling price, the day, the variable wm_yr_wk that contains year-week-code and the month. These variables were clearly the most important ones, but also the events and SNAP events did contribute which is interesting to note.\n",
    "\n",
    "<b> Autoregressive </b>\n",
    "\n",
    "Another 14 variables were added to mix,  in the form of lags from past autocorrelation of the target variable. These lags were added by looking at the PACF of sales in different stores. The RMSE improved with a decrease of 18 % from the previous step, to 1.377. The most important variables were the selling price, the sales lag from 1 day away, the id of the product, the day, and pretty much all the rest of the sales lag components that were created. We also notice that the date variables such as month and day of the week, now gained a higher importance compared to the previous step.\n",
    "\n",
    "<b> Moving-Average </b>\n",
    "\n",
    "Here, 28 variables were included in the form of lags that were calculated from the averages of sales in the past. The amount of lags was determined with the ACF of sales in different stores. We could see the model making it the most drastic improvement of all the steps by reaching RMSE of 0.09078 (93 % decrease from the previous steps). Looking at the most important features, we notice the rolling averages one to three days together with the sales lag from one and two days away. From the initial variables, we have the day of the week, the day, and the selling price that contributed for the accuracy of the forecast.\n",
    "\n",
    "<b> Trend </b>\n",
    "\n",
    "Two new trend variables were added to the model to take into account the increasing and decreasing trend of sales that was seen in the different stores. The model now further gained a 27 % decrease in the RMSE, by reaching 0.0660. What is interesting to see is that the sales averages that were included in the previous step, had a more important contribution now than previously. Besides the same sales lags and rolling averages that were important already in the previous step, also the two new selling trend variables contributed significantly to the accuracy of the model.\n",
    "\n",
    "<b> Seasonal </b>\n",
    "\n",
    "24 Seasonal variables to capture the low and high spikes from sales were now included in the model. The RMSE dropped a further 70 % from the previous, now reaching an error rate of 0.01936. Here the rolling maximum of the first 4 days and the rolling minimum of 2 days contributed to the accuracy of the model, together with the variables that were important already in the previous step.\n",
    "\n",
    "<b> Relative Price</b>\n",
    "\n",
    "For the last step, three variables depicting price elasticities and competition were added to the final model. The RMSE reached an error rate of 0.01915. The most important of the relative price variables seemed to be the price difference of the same products in other stores. This is the price that is set by each store individually, and it seems to have an effect on the consumer demand, based on these results.\n",
    "\n",
    "<b> Conclusion </b>\n",
    "\n",
    "With all the available features the model reached the highest rate of accuracy in the forecast. A closer look at the last step shows that the model utilized features from all the five different components that were created, and also from the variables that were given from the beginning. There were some issues related to the forecast error, especially in the second and third stores in California and the second store in Texas and Wisconsin, so next we will try to address these issues by fine-tuning the hyperparameters that will also give at the same time an opportunity to cross-validate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZnRER0k_saB"
   },
   "source": [
    "# 6. Fine-Tuning the Best Model\n",
    "\n",
    "As the results were quite good related to accuracy, we will focus on making sure the model doesn't overfit so that it manages to generalize also well with unseen data. For this, we will use a grid search that conducts an exhaustive search over all the combinations of the chosen hyperparameters related to overfitting, and combine it together with a K-fold Time Series -split for cross-validation. Split this way, enables the data to have fixed time intervals and the successive training sets to be supersets of those that come before them. \n",
    "\n",
    "We will perform the forecast similarly as before, by conducting a separate forecast for each store but this time we will add more regularization for the features as well as random sampling of the features. This forces the model to try different sets of features, and thus helps with not getting overfitted.\n",
    "\n",
    "## Grid Search Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JgEM5cU_saE",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.743526600Z"
    }
   },
   "outputs": [],
   "source": [
    "# GridSearchCV for each store individually with 5 K-fold\n",
    "\n",
    "stores = data.store_id.unique()\n",
    "d_store_id = dict_data['d_store_id']\n",
    "\n",
    "FOLDER_MODEL = 'data/sales_forecasting/cv/'\n",
    "Path(FOLDER_MODEL).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Saving results\n",
    "dict_cv = {}\n",
    "dict_params = {}\n",
    "dict_cv_features = {}\n",
    "dict_cv_results = {}\n",
    "df_preds = pd.DataFrame()\n",
    "\n",
    "for store in tqdm(stores): # Loop over stores and keep track of time\n",
    "    df = data.copy()\n",
    "        \n",
    "    df = df[df['store_id']==store] # Select store\n",
    "\n",
    "    X_train, y_train = df[df['d']<1914].drop('sales', axis=1), df[df['d']<1914]['sales'] \n",
    "\n",
    "        \n",
    "    X_test, y_test = df[(df['d']>=1914) & (df['d']<1942)].drop('sales', axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sales']\n",
    "    \n",
    "    # Search parameters\n",
    "    param_grid = [{ 'reg_lambda': [0.1, 0.2, 0.3, 0.4, 0.5], 'colsample_bytree': [0.6, 0.8] }]\n",
    "    \n",
    "    # The model\n",
    "    lgbm_reg = LGBMRegressor(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.08,\n",
    "            max_depth=8,\n",
    "            objective='poisson',\n",
    "            num_leaves=100,\n",
    "            min_child_weight=300,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "    # Cross-validation with Grid Search\n",
    "    cv = TimeSeriesSplit(n_splits=5).get_n_splits([X_train, y_train])    \n",
    "    grid_search = GridSearchCV(lgbm_reg, param_grid, scoring='neg_mean_squared_error', cv=cv)\n",
    "    grid_search.fit(X_train, y_train, verbose=0)\n",
    "    \n",
    "    # Prediction\n",
    "    y_preds = grid_search.predict(X_test)\n",
    "\n",
    "    # DataFrame for predictions    \n",
    "    df_cv = pd.DataFrame({\n",
    "            'testing':y_test,\n",
    "            'prediction':y_preds,\n",
    "            'store': d_store_id[store]\n",
    "        })\n",
    "    df_cv['error'] = df_cv['testing'] - df_cv['prediction']\n",
    "    df_cv['RMSE'] = np.sqrt(np.mean((df_cv.testing.values - df_cv.prediction.values) ** 2))/np.mean(df_cv.testing.values)\n",
    "    df_preds = pd.concat([df_preds, df_cv])\n",
    "    \n",
    "    # Save to dictionary   \n",
    "    dict_cv[store] = np.sqrt(-grid_search.best_score_)\n",
    "    dict_params[store] = grid_search.best_params_\n",
    "    dict_cv_features[store] = grid_search.best_estimator_.feature_importances_\n",
    "    dict_cv_results[store] = grid_search.cv_results_\n",
    "    \n",
    "    # Save the model\n",
    "    filename = FOLDER_MODEL + 'cv_model_' + str(d_store_id[store])+'.pkl'\n",
    "    joblib.dump(grid_search, filename)\n",
    "  \n",
    "    print('RMSE for {0} = {1}'.format(d_store_id[store], np.round_(np.sqrt(-grid_search.best_score_), 5)))\n",
    "    print('=' * 25)\n",
    "    print(grid_search.best_params_)\n",
    "    print('-' * 35)\n",
    "    \n",
    "    del grid_search, df, X_train, y_train, X_test, y_test\n",
    "\n",
    "# Save prediction for all stores\n",
    "df_preds.to_csv(FOLDER_MODEL + 'df_preds.csv')\n",
    "\n",
    "# CV results for all stores\n",
    "df_cv_results = pd.DataFrame({\n",
    "    'RMSE': [dict_cv[store] for store in stores],\n",
    "    'PARAMS': [dict_params[store] for store in stores],\n",
    "    'FEATURES': [dict_cv_features[store] for store in stores],\n",
    "    'RESULTS': [dict_cv_results[store] for store in stores]\n",
    "    })\n",
    "df_cv_results.to_csv(FOLDER_MODEL + 'df_cv_results.csv')\n",
    "\n",
    "# Average RMSE for all stores\n",
    "print('AVG RMSE = {}'.format(np.round(df_cv_results['RMSE'].mean(),5)))\n",
    "\n",
    "# Test RMSE for all stores\n",
    "print('TEST RMSE = {}'.format(np.round(df_preds['RMSE'].mean(),5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-IJ50xr_saE"
   },
   "source": [
    "## Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBT6h8k5_saE",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.744527100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting results for Testing RMSE\n",
    "\n",
    "df_preds.groupby('store')['RMSE'].mean().sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title('Testing RMSE for Stores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEgQ6MAY_saF",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.744527100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting forecast for sales total in each store\n",
    "\n",
    "total_store_sales = df_preds.groupby(['store'])['testing'].sum()\n",
    "predicted_store_sales = df_preds.groupby(['store'])['prediction'].sum()\n",
    "\n",
    "plt.plot(total_store_sales, marker = 'o', linestyle= '--', color='blue')\n",
    "plt.plot(predicted_store_sales, marker = 'x', linestyle = '--', color='red')\n",
    "plt.title('Total Sales and Sales Forecast in Stores')\n",
    "plt.show()\n",
    "print('Total Sales in Stores    : {}'.format(df_preds['testing'].sum()))\n",
    "print('Total Sales Predictions  : {}'.format(df_preds['prediction'].round().sum()))\n",
    "print('Total Sales Error        : {}'.format((df_preds['testing']-df_preds['prediction']).round().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uBtc_bO_saF",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.744527100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting validation error\n",
    "\n",
    "df_preds['error'] = df_preds['testing']-df_preds['prediction']\n",
    "df_preds['error'].plot()\n",
    "plt.title('Validation Error for Stores')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO0_Kb-X_saF",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.744527100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation error by store\n",
    "\n",
    "df_preds.groupby('store')['error'].plot(legend=True)\n",
    "plt.title('Validation Error in Stores')\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5TSSStx_saG",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.745526700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preds['error'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU8SkI_K_saG"
   },
   "source": [
    "### 1st Store in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCC7Dh-E_saG",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.745526700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 1st store in California\n",
    "\n",
    "plt.title('Testing Error for 1st Store in California')\n",
    "df_preds[df_preds['store'] == 'CA_1']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'CA_1']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'CA_1']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'CA_1']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2ah3tq-_saG"
   },
   "source": [
    "### 2nd Store in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkeYagut_saH",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.745526700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 2nd store in California\n",
    "\n",
    "plt.title('Testing Error for 2nd Store in California')\n",
    "df_preds[df_preds['store'] == 'CA_2']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'CA_2']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'CA_2']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'CA_2']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq3febNx_saH"
   },
   "source": [
    "### 3rd Store in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRrXQy_Y_saH",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.746527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 3rd store in California\n",
    "\n",
    "plt.title('Testing Error for 3rd Store in California')\n",
    "df_preds[df_preds['store'] == 'CA_3']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'CA_3']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'CA_3']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'CA_3']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tp_TqqA_saH"
   },
   "source": [
    "### 4th Store in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JY6ZLTa_saH",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.746527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 4th store in California\n",
    "\n",
    "plt.title('Testing Error for 4th Store in California')\n",
    "df_preds[df_preds['store'] == 'CA_4']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'CA_4']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'CA_4']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'CA_4']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsZ_-d_c_saI"
   },
   "source": [
    "### 1st Store in Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OzJAAOe_saI",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.746527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 1st store in Texas\n",
    "\n",
    "plt.title('Testing Error for 1st Store in Texas')\n",
    "df_preds[df_preds['store'] == 'TX_1']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'TX_1']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'TX_1']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'TX_1']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyjNXrWN_saI"
   },
   "source": [
    "### 2nd Store in Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsVIxMPj_saI",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.747526500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 2nd store in Texas\n",
    "\n",
    "plt.title('Testing Error for 2nd Store in Texas')\n",
    "df_preds[df_preds['store'] == 'TX_2']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'TX_2']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'TX_2']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'TX_2']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufDAaP-7_saI"
   },
   "source": [
    "### 3rd Store in Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6pdHlH0_saI",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.747526500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 3rd store in Texas\n",
    "\n",
    "plt.title('Testing Error for 3rd Store in Texas')\n",
    "df_preds[df_preds['store'] == 'TX_3']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'TX_3']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'TX_3']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'TX_3']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qauat3OA_saJ"
   },
   "source": [
    "### 1st Store in Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em7idCag_saJ",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.747526500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 1st store in Wisconsin\n",
    "\n",
    "plt.title('Testing Error for 1st Store in Wisconsin')\n",
    "df_preds[df_preds['store'] == 'WI_1']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'WI_1']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'WI_1']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'WI_1']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtUoQCnn_saJ"
   },
   "source": [
    "### 2nd Store in Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GrpRJGF_saJ",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.747526500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 2nd store in Wisconsin\n",
    "\n",
    "plt.title('Testing Error for 2nd Store in Wisconsin')\n",
    "df_preds[df_preds['store'] == 'WI_2']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'WI_2']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'WI_2']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'WI_2']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dswV8Qx_saJ"
   },
   "source": [
    "### 3rd Store in Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pab6Wisk_saJ",
    "ExecuteTime": {
     "start_time": "2023-06-02T13:54:00.748526800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting testing error for 3rd store in Wisconsin\n",
    "\n",
    "plt.title('Testing Error for 3rd Store in Wisconsin')\n",
    "df_preds[df_preds['store'] == 'WI_3']['error'].round().plot()\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "print('Total Sales       :   {}'.format(df_preds[df_preds['store'] == 'WI_3']['testing'].sum()))\n",
    "print('Total Prediction  :   {}'.format(df_preds[df_preds['store'] == 'WI_3']['prediction'].round().sum()))\n",
    "print('Total Error       :  {}'.format(df_preds[df_preds['store'] == 'WI_3']['error'].round().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGcRfC7I_saK"
   },
   "source": [
    "# 7. Drawing Conclusions\n",
    "\n",
    "The goal was to forecast 28 days of sales for 10 stores located in 3 States in the US. After first creating relevant features to extract sales patterns from the past, the LGBM model was able to reach an accuracy that predicted on average almost all future sales with an absolute error rate of only missing on average 27 sales out from total sales of 1,231,764.\n",
    "\n",
    "The results were then cross-validated with a 5 k-fold which also enabled to fine-tune the regularization terms of the model. This lead for the RMSE to decrease further to an average error of 0.00979 with an absolute forecast error of 24 sales on average. With the final model, the absolute errors for all the stores remained mostly in single digits which is remarkable if we think how many products are sold each day at each store. \n",
    "\n",
    "After achieving the great results, the forecast could be easily updated with new data without affecting the overall architecture or the features of the model. The LGBM model thus offers a powerful way to forecast accurately when dealing with large and complicated datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
